{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe46e84-94bc-4bef-b3a4-e06e269a204c",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "\n",
    "A decision tree classifier is a machine learning algorithm that is used for both classification and regression tasks. In the context of classification, I'll describe how it works.\n",
    "\n",
    "Decision Tree Structure:\n",
    "A decision tree is a tree-like structure composed of nodes, where each node represents a decision or a test on a particular attribute, each branch represents the outcome of the test, and each leaf node represents the class label or the decision taken after evaluating all the attributes along the path from the root to the leaf.\n",
    "\n",
    "Key Components:\n",
    "Root Node: The topmost node in the tree, representing the initial decision or test.\n",
    "\n",
    "Internal Nodes: Nodes that represent a decision or a test on a specific attribute.\n",
    "\n",
    "Branches: Outcomes of the test or decision at each internal node.\n",
    "\n",
    "Leaf Nodes: Terminal nodes that represent the final class label or decision.\n",
    "\n",
    "How it Works:\n",
    "Selecting the Best Attribute:\n",
    "\n",
    "At each internal node, the algorithm selects the best attribute to split the data based on certain criteria. Common criteria include information gain, Gini impurity, or gain ratio. These criteria measure how well a particular attribute separates the data into different classes.\n",
    "Splitting the Data:\n",
    "\n",
    "The selected attribute is used to split the dataset into subsets. Each subset corresponds to a unique value of the chosen attribute.\n",
    "Recursive Process:\n",
    "\n",
    "The process of selecting the best attribute and splitting the data is repeated recursively for each subset, creating sub-trees until a stopping condition is met. This stopping condition could be a predefined depth of the tree, a minimum number of samples per leaf, or other criteria.\n",
    "Leaf Nodes and Class Labels:\n",
    "\n",
    "Once the tree is built, the leaf nodes contain the final class labels. When a new instance is presented to the tree for classification, it traverses the tree from the root to a leaf node based on the attribute tests, and the class label of the corresponding leaf node is assigned to the instance.\n",
    "Example:\n",
    "Consider a decision tree for classifying whether a person plays golf or not based on weather conditions. The root node might test whether it's raining. If it is, the tree might branch into another node testing wind speed, and so on, until a leaf node is reached with a decision, e.g., \"Don't play golf.\"\n",
    "\n",
    "Advantages:\n",
    "Easy to understand and interpret.\n",
    "Requires little data preparation.\n",
    "Can handle both numerical and categorical data.\n",
    "Disadvantages:\n",
    "Prone to overfitting, especially when the tree is deep.\n",
    "Sensitive to noisy data.\n",
    "May not generalize well to unseen data.\n",
    "Pruning:\n",
    "Pruning is a technique used to address overfitting by removing parts of the tree that do not provide significant power in predicting target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8574a97-e249-459f-92cb-0aaf56dcfb72",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "Define the Problem: We start with defining the classification problem, which involves predicting the class labels of a set of input data points based on a set of features.\n",
    "\n",
    "Entropy: The first step in building a decision tree is to calculate the entropy of the dataset, which is a measure of the amount of uncertainty or randomness in the data. The entropy is defined as:\n",
    "\n",
    "entropy = -Σ(p_i * log2(p_i))\n",
    "where p_i is the probability of an instance belonging to class i.\n",
    "The entropy is maximum when the classes are equally distributed and minimum when all the instances belong to a single class.\n",
    "Information Gain: Next, we calculate the information gain of each feature, which measures how much the feature contributes to reducing the entropy. The information gain is defined as:\n",
    "information_gain = entropy(parent) - Σ((n_i / n) * entropy(child_i))\n",
    "where parent is the entropy of the parent node, child_i is the entropy of the i-th child node, and n_i and n are the number of instances in the i-th child node and the parent node, respectively.\n",
    "The feature with the highest information gain is selected as the splitting feature.\n",
    "Splitting: We split the dataset based on the selected feature and repeat steps 2-3 for each child node until we reach a stopping criterion.\n",
    "\n",
    "Stopping Criterion: The stopping criterion can be based on the maximum depth of the tree, the minimum number of instances in a leaf node, or other measures of model complexity.\n",
    "\n",
    "Classification: To classify a new instance, we start at the root node of the tree and follow the path down the tree based on the values of the features until we reach a leaf node. The class label of the leaf node is then assigned to the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282962e-e3b7-4724-afa9-878b9b521f34",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "\n",
    "A decision tree classifier can be used to solve a binary classification problem by making a sequence of decisions based on the values of input features to assign an instance to one of two classes. Here's a step-by-step explanation of how a decision tree is used for binary classification:\n",
    "\n",
    "1. Training Phase:\n",
    "Input Data:\n",
    "\n",
    "You start with a labeled training dataset where each instance has features (attributes) and a corresponding binary class label (e.g., 0 or 1).\n",
    "Building the Tree:\n",
    "\n",
    "The decision tree algorithm recursively selects the best features to split the data based on criteria like information gain or Gini impurity.\n",
    "The tree is built by creating nodes at each decision point, branching based on feature values, and assigning class labels to leaf nodes.\n",
    "2. Decision Making:\n",
    "Traversal:\n",
    "\n",
    "To classify a new instance, you start at the root of the tree and traverse down to a leaf node.\n",
    "At each internal node, you evaluate the feature specified by the node and follow the branch corresponding to the value of that feature for the instance.\n",
    "Leaf Node Assignment:\n",
    "\n",
    "When you reach a leaf node, the class label associated with that leaf node is the predicted class for the input instance.\n",
    "\n",
    "3. Prediction:\n",
    "Application to New Data:\n",
    "\n",
    "Once the tree is built, you can use it to classify new, unseen instances by following the decision paths down to the leaf nodes.\n",
    "Output:\n",
    "\n",
    "The output of the decision tree for a binary classification problem is the predicted class label (0 or 1) assigned to the input instance.\n",
    "Advantages of Decision Trees for Binary Classification:\n",
    "Interpretability: Decision trees are easy to understand and interpret, making them useful for explaining the reasoning behind a classification decision.\n",
    "\n",
    "Versatility: They can handle both numerical and categorical data.\n",
    "\n",
    "Feature Importance: Decision trees can provide information about the importance of different features in the classification process.\n",
    "\n",
    "Limitations:\n",
    "Overfitting: Decision trees can be prone to overfitting, especially if the tree is deep and captures noise in the training data.\n",
    "\n",
    "Sensitivity to Variations: Small changes in the training data may result in different trees, making them sensitive to variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161ecf4-f477-4a3e-ad87-6f53d0f253df",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "\n",
    "The geometric intuition behind decision tree classification involves representing the decision boundaries of the classes in the feature space as a series of axis-aligned splits. Each split corresponds to a decision made based on a particular feature, and the final decision is determined by the region of the feature space in which a data point falls. Let's break down the geometric intuition and how it leads to predictions:\n",
    "\n",
    "1. Feature Space Partitioning:\n",
    "In a binary classification problem, the feature space is divided into regions or partitions based on the values of input features.\n",
    "\n",
    "Each internal node in the decision tree corresponds to a decision point, which can be visualized as a split along one of the features.\n",
    "\n",
    "The splits are typically orthogonal to the feature axes, resulting in axis-aligned decision boundaries.\n",
    "\n",
    "2. Decision Regions:\n",
    "The regions created by the splits define decision regions in the feature space, and each region is associated with a specific class label.\n",
    "\n",
    "As you traverse the tree from the root to a leaf, you are essentially moving through different decision regions.\n",
    "\n",
    "\n",
    "The geometric intuition behind decision tree classification involves representing the decision boundaries of the classes in the feature space as a series of axis-aligned splits. Each split corresponds to a decision made based on a particular feature, and the final decision is determined by the region of the feature space in which a data point falls. Let's break down the geometric intuition and how it leads to predictions:\n",
    "\n",
    "1. Feature Space Partitioning:\n",
    "In a binary classification problem, the feature space is divided into regions or partitions based on the values of input features.\n",
    "\n",
    "Each internal node in the decision tree corresponds to a decision point, which can be visualized as a split along one of the features.\n",
    "\n",
    "The splits are typically orthogonal to the feature axes, resulting in axis-aligned decision boundaries.\n",
    "\n",
    "2. Decision Regions:\n",
    "The regions created by the splits define decision regions in the feature space, and each region is associated with a specific class label.\n",
    "\n",
    "As you traverse the tree from the root to a leaf, you are essentially moving through different decision regions.\n",
    "\n",
    "Example:\n",
    "Consider a simple 2D feature space with two features, X-axis and Y-axis. The decision tree might make splits based on the values of these features:\n",
    "\n",
    "\n",
    "                 Y-axis\n",
    "                  |\n",
    "           [Split on X]\n",
    "           /             \\\n",
    "  [Class 0]           [Split on Y]\n",
    "                     /             \\\n",
    "                [Class 1]        [Class 0]\n",
    "The first split along the X-axis divides the space into two regions.\n",
    "The second split along the Y-axis further divides one of the regions into two sub-regions.\n",
    "3. Decision Making:\n",
    "To classify a new instance, you start at the root and follow the decision path based on the values of its features.\n",
    "\n",
    "At each internal node, you decide which branch to take based on whether the feature value is above or below a certain threshold.\n",
    "\n",
    "The final decision is made at a leaf node, where the instance falls into a specific decision region.\n",
    "\n",
    "4. Visualizing Decision Boundaries:\n",
    "The decision boundaries in the feature space are essentially the borders between different decision regions.\n",
    "\n",
    "These decision boundaries are straight lines or hyperplanes parallel to the coordinate axes, reflecting the axis-aligned splits made by the decision tree.\n",
    "\n",
    "5. Predictions:\n",
    "Once the decision tree is trained and the feature space is partitioned, predicting the class of a new instance involves determining which decision region it falls into.\n",
    "\n",
    "The class label associated with the leaf node corresponding to that region is the predicted class for the instance.\n",
    "\n",
    "Advantages of Geometric Intuition:\n",
    "Interpretability: The geometric representation of decision boundaries makes it easy to understand and interpret the decision-making process.\n",
    "\n",
    "Visualization: Decision trees provide a visually intuitive way to understand how the model is making predictions in the feature space.\n",
    "\n",
    "Limitations:\n",
    "Complex Decision Boundaries: While decision trees are powerful, they might struggle to capture complex decision boundaries that require non-axis-aligned splits.\n",
    "\n",
    "Overfitting: Deep decision trees can overfit the training data and create overly complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9b0fb-a425-4933-9b36-9fcf632c68a9",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "\n",
    "The confusion matrix is a table used in classification to evaluate the performance of a machine learning model. It provides a summary of the predicted and actual class labels for a classification problem. The matrix is particularly useful for understanding the types and frequencies of errors made by the model.\n",
    "\n",
    "Components of the Confusion Matrix:\n",
    "Let's define the components using a binary classification scenario:\n",
    "\n",
    "True Positive (TP): Instances that are actually positive and are correctly predicted as positive by the model.\n",
    "\n",
    "True Negative (TN): Instances that are actually negative and are correctly predicted as negative by the model.\n",
    "\n",
    "False Positive (FP): Instances that are actually negative but are incorrectly predicted as positive by the model (Type I error).\n",
    "\n",
    "False Negative (FN): Instances that are actually positive but are incorrectly predicted as negative by the model (Type II error).\n",
    "\n",
    "\n",
    "How to Use the Confusion Matrix for Model Evaluation:\n",
    "High Accuracy, but...:\n",
    "\n",
    "High accuracy alone may not be sufficient. Examine precision and recall to understand the trade-off between false positives and false negatives.\n",
    "Precision-Recall Trade-off:\n",
    "\n",
    "Precision is usually important when the cost of false positives is high.\n",
    "Recall is crucial when the cost of false negatives is high.\n",
    "Imbalanced Classes:\n",
    "\n",
    "In cases of imbalanced classes, accuracy might be misleading. Focus on precision and recall for a more comprehensive evaluation.\n",
    "Receiver Operating Characteristic (ROC) Curve:\n",
    "\n",
    "ROC curve is a graphical representation of the trade-off between sensitivity and specificity at various thresholds. The Area Under the Curve (AUC) is also used as a metric.\n",
    "Adjusting Decision Threshold:\n",
    "\n",
    "Depending on the problem, you might adjust the decision threshold to balance precision and recall according to your specific requirements.\n",
    "Example Interpretation:\n",
    "Consider a medical diagnosis scenario:\n",
    "\n",
    "Accuracy: Overall percentage of correct predictions.\n",
    "Precision: Proportion of predicted positive cases that are actually positive (minimizing false positives, which can be costly in healthcare).\n",
    "Recall: Proportion of actual positive cases that are correctly predicted (minimizing false negatives to catch all potential cases).\n",
    "In healthcare, a false negative might mean missing a patient who needs treatment, while a false positive might lead to unnecessary treatments. The confusion matrix helps in understanding and optimizing these trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c103da1-0c77-46bf-8d7e-a4f4a0b15dbf",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Let's consider a binary classification problem where the task is to predict whether an email is spam or not spam (ham). Here's a hypothetical confusion matrix based on the model's predictions and the actual outcomes:\n",
    "\n",
    "                    Actual Spam    Actual Ham\n",
    "Predicted Spam          90             10\n",
    "Predicted Ham           15             385\n",
    "\n",
    "\n",
    "\n",
    "Let's consider a binary classification problem where the task is to predict whether an email is spam or not spam (ham). Here's a hypothetical confusion matrix based on the model's predictions and the actual outcomes:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "                    Actual Spam    Actual Ham\n",
    "Predicted Spam          90             10\n",
    "Predicted Ham           15             385\n",
    "In this confusion matrix:\n",
    "\n",
    "True Positive (TP): 90 (Predicted Spam and actually Spam)\n",
    "True Negative (TN): 385 (Predicted Ham and actually Ham)\n",
    "False Positive (FP): 10 (Predicted Spam but actually Ham)\n",
    "False Negative (FN): 15 (Predicted Ham but actually Spam)\n",
    "\n",
    "\n",
    "Now, let's calculate precision, recall, and F1 score:\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Using the values from the confusion matrix, we can calculate the precision, recall, and F1 score as follows:\n",
    "Precision = 20 / (20 + 5) = 0.80\n",
    "Recall = 20 / (20 + 10) = 0.67\n",
    "F1 Score = 2 * (0.80 * 0.67) / (0.80 + 0.67) = 0.73\n",
    "\n",
    "So, in this example:\n",
    "The precision of the classifier is 0.80, which means that out of all the patients that the classifier predicted to have the disease, 80% actually had the disease.\n",
    "The recall of the classifier is 0.67, which means that out of all the patients who actually had the disease, the classifier correctly identified 67% of them.\n",
    "The F1 score is 0.73, which is a weighted average of precision and recall and provides an overall measure of the classifier's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3950961a-613c-4a09-bf97-b19c6f4c7c0b",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "Choosing an appropriate evaluation metric for a classification problem is crucial because different metrics highlight different aspects of model performance, and the choice depends on the specific goals and requirements of the task. Here are some key considerations and steps to help select an appropriate evaluation metric:\n",
    "\n",
    "1. Understand the Problem and Stakeholders:\n",
    "Class Imbalance: If the classes are imbalanced, accuracy might not be an informative metric. For example, in fraud detection, where fraudulent transactions are rare, a model predicting all transactions as non-fraudulent can have high accuracy but is not useful.\n",
    "\n",
    "Stakeholder Preferences: Consider the relative importance of false positives and false negatives. In medical diagnoses, for instance, the cost of missing a positive case (false negative) might be much higher than misclassifying a negative case (false positive).\n",
    "\n",
    "2. Define the Business Goal:\n",
    "Define Success: Clearly define what success looks like in the context of the problem. This may involve minimizing false positives, maximizing true positives, achieving a balance, or optimizing for precision or recall.\n",
    "3. Select Relevant Metrics:\n",
    "Accuracy: Suitable for balanced datasets but can be misleading in imbalanced scenarios.\n",
    "Accuracy\n",
    "=\n",
    "TP\n",
    "+\n",
    "TN\n",
    "TP\n",
    "+\n",
    "FP\n",
    "+\n",
    "FN\n",
    "+\n",
    "TN\n",
    "Accuracy= \n",
    "TP+FP+FN+TN\n",
    "TP+TN\n",
    "​\n",
    " \n",
    "\n",
    "Precision: Emphasizes minimizing false positives.\n",
    "Precision\n",
    "=\n",
    "TP\n",
    "TP\n",
    "+\n",
    "FP\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " \n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Emphasizes minimizing false negatives.\n",
    "Recall\n",
    "=\n",
    "TP\n",
    "TP\n",
    "+\n",
    "FN\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "\n",
    "Specificity (True Negative Rate): Emphasizes minimizing false positives.\n",
    "Specificity\n",
    "=\n",
    "TN\n",
    "TN\n",
    "+\n",
    "FP\n",
    "Specificity= \n",
    "TN+FP\n",
    "TN\n",
    "​\n",
    " \n",
    "\n",
    "F1 Score: Balances precision and recall.\n",
    "F1 Score\n",
    "=\n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1 Score=2× \n",
    "Precision+Recall\n",
    "Precision×Recall\n",
    "​\n",
    " \n",
    "\n",
    "Area Under the Receiver Operating Characteristic Curve (AUC-ROC): Suitable for evaluating the trade-off between sensitivity and specificity at various thresholds.\n",
    "\n",
    "4. Consider the Context:\n",
    "Domain Knowledge: Consider the domain-specific knowledge and the practical implications of model predictions.\n",
    "\n",
    "Legal and Ethical Implications: In some cases, certain types of errors might have legal or ethical consequences.\n",
    "\n",
    "5. Use Multiple Metrics:\n",
    "Comprehensive Evaluation: Using multiple metrics provides a more comprehensive understanding of the model's performance.\n",
    "\n",
    "Threshold Analysis: Evaluate how metrics change at different decision thresholds.\n",
    "\n",
    "6. Monitor Over Time:\n",
    "Dynamic Environment: In dynamic environments, where the data distribution may change over time, regularly monitor and update evaluation metrics.\n",
    "7. Validation and Cross-Validation:\n",
    "Validation Set: Use a separate validation set to assess the model's generalization performance.\n",
    "\n",
    "Cross-Validation: Perform cross-validation to obtain a more robust estimate of the model's performance.\n",
    "\n",
    "Example:\n",
    "Consider a credit scoring model:\n",
    "\n",
    "Goal: Minimize the number of false positives (approving a high-risk customer).\n",
    "\n",
    "Metric Choice: Precision might be more important than recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc9619-8b63-48ee-bb2a-bb558fed0afa",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "A classic example where precision is a critical metric is in the context of email spam filtering. In this scenario, the goal is to identify and filter out spam emails while minimizing the number of legitimate (non-spam) emails incorrectly classified as spam, also known as false positives.\n",
    "\n",
    "Example: Email Spam Filtering\n",
    "Goal:\n",
    "Minimize the number of legitimate emails marked as spam to ensure that important communications are not mistakenly filtered out.\n",
    "\n",
    "Importance of Precision:\n",
    "Precision Definition:\n",
    "Precision\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Positives\n",
    "Precision= \n",
    "True Positives + False Positives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "Context:\n",
    "False Positives (FP) in this context correspond to legitimate emails being incorrectly classified as spam.\n",
    "A high precision means that the spam filter has a low rate of marking legitimate emails as spam.\n",
    "Explanation:\n",
    "Consequences of False Positives:\n",
    "\n",
    "If a legitimate email is wrongly marked as spam, it may lead to missed opportunities, business communications being ignored, or important notifications going unnoticed.\n",
    "User Experience:\n",
    "\n",
    "False positives can be particularly frustrating for users who rely on their email for important communication. If a user consistently finds important emails in the spam folder, they might lose trust in the spam filter.\n",
    "Balancing Act:\n",
    "\n",
    "While it's important to filter out spam, striking a balance is crucial to avoid inconveniencing users with an excessive number of false positives.\n",
    "Preventing Information Loss:\n",
    "\n",
    "In certain contexts, the consequences of missing an important email (false negative) might be less severe than marking an important email as spam. Users may prefer to manually check their spam folder occasionally rather than risk losing critical communications.\n",
    "Precision-Recall Trade-off:\n",
    "\n",
    "While precision is emphasized in this example, it's important to acknowledge the trade-off with recall. Emphasizing precision may lead to an increase in false negatives (legitimate emails marked as spam), so the trade-off should be carefully considered based on user expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0fb7c-d87a-48e8-a755-1f8598c99fca",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "\n",
    "A classic example where recall is the most important metric is in the context of medical diagnoses, particularly when dealing with life-threatening conditions. In this scenario, the primary goal is to identify and correctly classify all instances of the positive class (e.g., detecting a disease), even if it comes at the cost of a higher number of false positives.\n",
    "\n",
    "Example: Medical Diagnoses for a Rare Disease\n",
    "Goal:\n",
    "Maximize the detection of individuals with a rare but life-threatening disease to ensure early intervention and treatment.\n",
    "\n",
    "Importance of Recall:\n",
    "Recall Definition:\n",
    "Recall\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Negatives\n",
    "Recall= \n",
    "True Positives + False Negatives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "Context:\n",
    "False Negatives (FN) in this context correspond to individuals with the disease who are incorrectly classified as not having the disease.\n",
    "A high recall means that the model is successful in identifying a significant portion of individuals with the disease.\n",
    "Explanation:\n",
    "Life-Threatening Consequences:\n",
    "\n",
    "In cases of life-threatening diseases, early detection and intervention are crucial for effective treatment and improved outcomes. Missing a positive case (false negative) can have severe consequences for the patient.\n",
    "Prioritizing Sensitivity:\n",
    "\n",
    "Maximizing recall ensures that the model is sensitive to the presence of the disease. It aims to capture as many true positives as possible, even if it comes at the cost of more false positives.\n",
    "Reducing False Negatives:\n",
    "\n",
    "False negatives, in this context, mean failing to diagnose a person who actually has the disease. This is a critical error that could result in delayed treatment and worsened patient outcomes.\n",
    "Trade-off with Precision:\n",
    "\n",
    "Emphasizing recall may lead to an increase in false positives, as the model may be more inclusive in classifying individuals as potentially having the disease. However, in the context of a life-threatening condition, the emphasis is on minimizing false negatives.\n",
    "Public Health Impact:\n",
    "\n",
    "In public health scenarios, maximizing recall is often a priority to prevent the spread of infectious diseases or to identify and manage outbreaks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0552b-a41f-4ba3-adc8-f0f5464d2d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
